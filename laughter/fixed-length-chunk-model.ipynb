{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friends canned laughter identification\n",
    "\n",
    "In this notebook, we will try to use data from available friends\n",
    "episodes to try to train a model using VGGish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils/')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import utils\n",
    "import episode\n",
    "import color\n",
    "import stats\n",
    "# stdlib and package imports\n",
    "import numpy as np\n",
    "from pathlib import Path \n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "# keras and ML imports\n",
    "import talos\n",
    "from keras.models import Sequential, Model, model_from_yaml\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize as sknormalize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn import under_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting data segments\n",
    "Here we will extract labelled embeddings of wav data for each of the episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the episodes we have annotation data for\n",
    "episodes = ['friends-s02-e{:0>2d}'.format(i) for i in range(1, 5)] + ['friends-s03-e09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use VGGish to generate embeddings for each of the episode, and split data into chunks of 0.96s (no preserve_length)\n",
    "# see if archive exists so the expensive method is run only if not run already\n",
    "X_raw, Y_raw, refs = episode.get_data(which_episodes=episodes, use_vggish=True, preserve_length=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw.shape, Y_raw.shape, refs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_raw[-4], refs[-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we'll use the extracted data to generate balanced training and testing data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, resample data to have equal number of 'laugh' and 'no-laugh' examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = under_sampling.RandomUnderSampler(sampling_strategy='not minority')\n",
    "X_res, Y_res = rus.fit_resample(X_raw, Y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# did the resampling work?\n",
    "Counter(Y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, split data into training and testing sets so it doesn't get mixed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_res, Y_res, test_size=0.25, random_state=1,\n",
    "                                                    stratify=Y_res.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again: did the split go as expected?\n",
    "Counter(Y_train), Counter(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we'll attempt to model the balanced data using a Keras dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(128,), name='in0')\n",
    "\n",
    "layer = Dense(16, activation='relu', name='d0')(inp)\n",
    "layer = Dropout(.4, name='dr0')(layer)\n",
    "\n",
    "layer = Dense(8, activation='relu', name='d1')(layer)\n",
    "layer = Dropout(.4, name='dr1')(layer)\n",
    "\n",
    "layer = Dense(1, activation='sigmoid', name='out')(layer)\n",
    "\n",
    "model = Model(inputs=[inp], outputs=[layer])\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "H = model.fit(X_train, Y_train.reshape(-1), epochs=32, validation_data=[X_test, Y_test.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "stats.plot_history(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC curve for this model and data\n",
    "stats.plot_roc_curve(model=model, x=X_test, y_true=Y_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per episode train/valid split (Taylor)\n",
    "\n",
    "The above code is cheating a bit by taking one laugh incident and allowing\n",
    "adjacent chunks to be in both the training and validation sets. It's easy\n",
    "to fix this though by constructing the training and testing sets from entirely\n",
    "different episodes. This also guards against the case that laughter is different\n",
    "in some episodes compared to others.\n",
    "\n",
    "Oddly enough, the results seem to perform worse on the training data but just as\n",
    "well on the validation set. Though these results seem to depend a lot on how long\n",
    "the model is trained for... Hopefully more tuning will fix this. Will have to think\n",
    "about why that is the case, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_num = [int(x[13:15]) for x, _, _ in refs]\n",
    "train_flag = np.array([x in [1, 3, 9] for x in eps_num])\n",
    "print(Counter(train_flag))\n",
    "\n",
    "X_raw_train = X_raw[train_flag,]\n",
    "X_raw_valid = X_raw[~train_flag,]\n",
    "Y_raw_train = Y_raw[train_flag,]\n",
    "Y_raw_valid = Y_raw[~train_flag,]\n",
    "\n",
    "rus = under_sampling.RandomUnderSampler(sampling_strategy='not minority')\n",
    "X_train, Y_train = rus.fit_resample(X_raw_train, Y_raw_train)\n",
    "X_valid, Y_valid = rus.fit_resample(X_raw_valid, Y_raw_valid)\n",
    "\n",
    "print(Counter(Y_train))\n",
    "print(Counter(Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(128,), name='in0')\n",
    "\n",
    "layer = Dense(16, activation='relu', name='d0')(inp)\n",
    "layer = Dropout(.4, name='dr0')(layer)\n",
    "\n",
    "layer = Dense(8, activation='relu', name='d1')(layer)\n",
    "layer = Dropout(.4, name='dr1')(layer)\n",
    "\n",
    "layer = Dense(1, activation='sigmoid', name='out')(layer)\n",
    "\n",
    "model = Model(inputs=[inp], outputs=[layer])\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(X_train, Y_train.reshape(-1), epochs=32, validation_data=[X_valid, Y_valid.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_valid) > 0.5\n",
    "confusion_matrix(Y_valid, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot_history(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot_roc_curve(model=model, x=X_raw_valid, y_true=Y_raw_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per season train/valid split\n",
    "\n",
    "In the code block below, we'll try to train our model on data from one season,\n",
    "and validate it on data from a whole different season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "szn_num = [int(x[9:11]) for x, _, _ in refs]\n",
    "train_flag = np.array([x in [2] for x in szn_num])\n",
    "print(Counter(train_flag))\n",
    "\n",
    "X_raw_train = X_raw[train_flag,]\n",
    "X_raw_valid = X_raw[~train_flag,]\n",
    "Y_raw_train = Y_raw[train_flag,]\n",
    "Y_raw_valid = Y_raw[~train_flag,]\n",
    "\n",
    "rus = under_sampling.RandomUnderSampler(sampling_strategy='not minority')\n",
    "X_train, Y_train = rus.fit_resample(X_raw_train, Y_raw_train)\n",
    "X_valid, Y_valid = rus.fit_resample(X_raw_valid, Y_raw_valid)\n",
    "\n",
    "print(Counter(Y_train))\n",
    "print(Counter(Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(128,), name='in0')\n",
    "\n",
    "layer = Dense(16, activation='relu', name='d0')(inp)\n",
    "layer = Dropout(.4, name='dr0')(layer)\n",
    "\n",
    "layer = Dense(8, activation='relu', name='d1')(layer)\n",
    "layer = Dropout(.4, name='dr1')(layer)\n",
    "\n",
    "layer = Dense(1, activation='sigmoid', name='out')(layer)\n",
    "\n",
    "model = Model(inputs=[inp], outputs=[layer])\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(X_train, Y_train.reshape(-1), epochs=32, validation_data=[X_valid, Y_valid.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_valid) > 0.5\n",
    "confusion_matrix(Y_valid, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot_history(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot_roc_curve(model=model, x=X_raw_valid, y_true=Y_raw_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll try to move around in the variable space and see what variables produce optimal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dense0': (13, 21, 5),\n",
    "          'drop0': [.1, .2, .3, .4, .5],\n",
    "          'act': ['relu', 'tanh'],\n",
    "          'dense1': (5, 11, 4),\n",
    "          'drop1': [.1, .2, .3, .4, .5],\n",
    "          'batch_size': [8, 16, 32],\n",
    "          'optimizer': ['rmsprop']}\n",
    "\n",
    "\n",
    "def laugh_model(x, y, x_val, y_val, params):\n",
    "    inp = Input(shape=(128,), name='in0')\n",
    "\n",
    "    layer = Dense(params['dense0'], activation=params['act'], name='d0')(inp)\n",
    "    layer = Dropout(params['drop0'], name='dr0')(layer)\n",
    "\n",
    "    layer = Dense(params['dense1'], activation=params['act'], name='d1')(layer)\n",
    "    layer = Dropout(params['drop1'], name='dr1')(layer)\n",
    "\n",
    "    layer = Dense(1, activation='sigmoid', name='out')(layer)\n",
    "    \n",
    "    model = Model(inputs=[inp], outputs=[layer])\n",
    "    model.compile(optimizer=params['optimizer'], loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "                           \n",
    "    H = model.fit(x, y.reshape(-1), epochs=30, validation_data=[x_val, y_val.reshape(-1)], verbose=0,\n",
    "                  batch_size=params['batch_size'])\n",
    "\n",
    "    return H, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = talos.Scan(x=X_train, y=Y_train, x_val=X_valid, y_val=Y_valid, params=params, model=laugh_model, grid_downsample=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = talos.Reporting(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_vals = r.data.sort_values(by=['val_binary_accuracy'], ascending=0).head(10)\n",
    "opt_vals.to_csv('../data/tuning_optimum.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
