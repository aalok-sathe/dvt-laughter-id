{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canned laughter identification model using spectrograms of sound\n",
    "\n",
    "In this notebook, we will train a model based on friends laughter/non-laughter spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "import utils\n",
    "%aimport soundutils\n",
    "%aimport episode\n",
    "import color\n",
    "import stats\n",
    "%aimport modelbuilder\n",
    "# stdlib and package imports\n",
    "import pydub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from collections import Counter\n",
    "from scipy import signal\n",
    "# keras and ML imports\n",
    "from keras.models import Sequential, Model, model_from_yaml\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize as sknormalize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn import under_sampling\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrograms\n",
    "In this notebook we'll extract spectrograms of sound data and try to train a model using these instead of the raw audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Explore `scipy.signal.spectrogram` and write useful wrapper methods in our library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = Path('../wav/') / 'friends-s03-e09.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, wav = wavfile.read(str(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = 9 # 9th chunk\n",
    "wid = 1 # 1 second width\n",
    "f, t, Sxx = signal.spectrogram(wav[win*wid*sr:(win+1)*wid*sr], sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the default output produced by scipy would look like. However, we don't want so much data,\n",
    "so we'll use a wrapper to downsample by smoothing it over sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.pcolormesh(t, f[:100], np.log10(1+Sxx[:100]))\n",
    "plt.ylabel('log Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test a wrapper method\n",
    "`soundutils.get_data_spectro`:\n",
    "- `wavdata`: raw wav audio data\n",
    "- `sr`: sampling rate of wav\n",
    "- `windowlen`: length of sliding window to smooth over (milliseconds)\n",
    "- `fn`: a function that will take an (n, windowlen) np array and return a (n,) np array which is the smoothed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, samples = soundutils.get_data_spectro(wavdata=wav[win*wid*sr:(win+1)*wid*sr], sr=sr, windowlen=16,\n",
    "                                            fn=lambda x: np.mean(x, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see a smoother plot than before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.pcolormesh(t, f, samples.T)\n",
    "plt.ylabel('log Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with a method in place, we try it out with a whole episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the episodes we have annotation data for\n",
    "episodes = ['friends-s02-e{:0>2d}'.format(i) for i in range(1, 5)] + ['friends-s03-e09']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, Y_raw, refs = episode.get_data(which_episodes=episodes, backend='spectro', task='laughter', windowlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(145, 155):\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.pcolormesh(np.array(X_raw[0:3000]).T.reshape(129,-1)[:100])\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.pcolormesh(np.array(X_raw[11401:14401]).T.reshape(129,-1)[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we'll use the extracted data to generate balanced training and testing data sets\n",
    "First, resample data to have equal number of 'laugh' and 'no-laugh' examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "szn_num = [int(x[9:11]) for x, _, _ in refs]\n",
    "train_flag = np.array([x in [2] for x in szn_num])\n",
    "print(Counter(train_flag))\n",
    "\n",
    "X_raw_train = X_raw[train_flag,]\n",
    "X_raw_valid = X_raw[~train_flag,]\n",
    "Y_raw_train = Y_raw[train_flag,]\n",
    "Y_raw_valid = Y_raw[~train_flag,]\n",
    "\n",
    "rus = under_sampling.RandomUnderSampler(sampling_strategy='not minority')\n",
    "X_train, Y_train = rus.fit_resample(X_raw_train, Y_raw_train)\n",
    "X_valid, Y_valid = rus.fit_resample(X_raw_valid, Y_raw_valid)\n",
    "\n",
    "print(Counter(Y_train.flatten()))\n",
    "print(Counter(Y_valid.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(*X_train.shape, 1)\n",
    "X_valid = X_valid.reshape(*X_valid.shape, 1)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we'll attempt to model the balanced data using a Convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='task:{task}-spectro-ckpt.hdf5'.format(task='train-on-s02'),\n",
    "                             save_best_only=True)\n",
    "model = modelbuilder.build_conv_model(optimizer='rmsprop', drop1=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "H = model.fit(X_train, Y_train.reshape(-1), epochs=10, \n",
    "              validation_data=[X_valid, Y_valid.reshape(-1)], callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot_history(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a logistic regression model\n",
    "\n",
    "How well does a logistic regression model do in making predictions\n",
    "of laughter with the spectrogram data? I'll fit a model with an l1\n",
    "penalty (lasso regression) and set the penalty parameter to a relatively\n",
    "small value (C=0.01) corresponding with a high amount of penalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegression(penalty=\"l1\", C=0.01)\n",
    "model = model.fit(X_train[:, :, 0], Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does very similarly to the neural network model. It has about\n",
    "a 76% classification rate. You can get a bit higher with a larger value\n",
    "of C (around 78%), but I like the small model for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_valid[:, :, 0], Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_logit = model.predict_proba(X_valid[:, :, 0])[:,1]\n",
    "fpr, tpr, thr = sklearn.metrics.roc_curve(Y_valid, yhat_logit)\n",
    "plt.plot(fpr, tpr, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the coefficents. Notice that only the first 30ish values come\n",
    "into the model; this matches what we saw when looking at the data manually\n",
    "(i.e., the signal comes from the smaller frequencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice that the model is doing well balancing the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_logit = model.predict_proba(X_valid[:, :, 0])[:,1]\n",
    "_ = plt.hist(yhat_logit, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How correlated are the sets of predictions? Relatively high, but\n",
    "not overly so (r=0.84 on my run of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_keras = H.model.predict(X_valid[:, :, :])[:,0]\n",
    "yhat_logit = model.predict_proba(X_valid[:, :, 0])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(yhat_keras, yhat_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(yhat_keras > 0.5, yhat_logit > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code should not be trusted\n",
    "\n",
    "Saving the code below, but doesn't mean much unless we can sort the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(yhat_keras[1:], yhat_keras[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(yhat_logit[1:], yhat_logit[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothed NN model\n",
    "for size in [1, 5, 10, 50, 100, 500, 1000]:\n",
    "    weights = [1 / size] * size\n",
    "    yhat_keras_smooth = np.convolve(yhat_keras, np.array(weights)[::-1],'same')\n",
    "    acc = np.mean(np.int32(yhat_keras_smooth > 0.5) == np.int32(Y_valid[:,0]))\n",
    "    print(\"size = {0: 5d}    acc = {1:01.04f}\".format(size, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothed Logistic Model\n",
    "for size in [1, 5, 10, 50, 100, 500, 1000, 6500]:\n",
    "    weights = [1 / size] * size\n",
    "    yhat_logit_smooth = np.convolve(yhat_logit, np.array(weights)[::-1], 'same')\n",
    "    acc = np.mean(np.int32(yhat_logit_smooth > 0.5) == np.int32(Y_valid[:,0]))\n",
    "    \n",
    "    print(\"size = {0: 5d}    acc = {1:01.04f}\".format(size, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
